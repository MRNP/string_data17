{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# igraph is a library compatible with Python, R, and C/C++\n",
    "# We will use Python in this tutorial\n",
    "\n",
    "from igraph import *\n",
    "from random import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a graph object\n",
    "g = Graph()\n",
    "\n",
    "# This graph has 'N' nodes\n",
    "N = 20\n",
    "\n",
    "# Each pair of nodes is connected with probability 'p'\n",
    "p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first graph we study is the Erdős-Renyi random graph, often our 'null model'\n",
    "\n",
    "g.add_vertices(N) # Add N edges to our graph\n",
    "\n",
    "# Add edges with probability 'p'\n",
    "edges = []\n",
    "for i in range(N):\n",
    "    for j in range(i + 1, N):\n",
    "        if random() < p:\n",
    "            edges += [(i, j)]\n",
    "g.add_edges(edges)\n",
    "print('This graph has [%d] edges.' % g.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's visualize this graph!\n",
    "\n",
    "g.write_svg(\"graph.svg\", layout=g.layout_circle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's look at some structural properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a larger Erdős-Renyi random graph (for nicer statistics)\n",
    "N = 1000\n",
    "p = 0.01\n",
    "G = Graph.Erdos_Renyi(N, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the mean number of edges per node\n",
    "\n",
    "degrees = G.degree()\n",
    "print('The average degree is [%d].' % mean(degrees))\n",
    "print('The maximum degree is [%d].' % max(degrees))\n",
    "print('This graph has [%d] links.' % G.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are also interested in the probability distribution of degrees\n",
    "n, bins, hist = plt.hist(degrees, bins=30, normed=1)\n",
    "\n",
    "# Ansatz: This is a Poisson distribution\n",
    "y = mlab.normpdf(bins, mean(degrees), math.sqrt(mean(degrees)))\n",
    "l = plt.plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "plt.xlabel('Degree (k)')\n",
    "plt.ylabel('Frequency P(k)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Real-world networks follow a Preferential Attachment model developed by Barabasi and Albert (1999)\n",
    "\n",
    "# Start with a complete graph of 'm0' nodes\n",
    "m0 = 10\n",
    "B = Graph.Full(m0)\n",
    "\n",
    "# Each new node connects to 'm' nodes in the existing graph\n",
    "m = 5\n",
    "\n",
    "# The probability the new node connects to each old node 'i' is p(i) = degree(i) / sum(degrees)\n",
    "def pref_attach_prob(g):\n",
    "    d = g.degree()\n",
    "    return np.cumsum([x / sum(d) for x in d])\n",
    "\n",
    "# We will grow the graph to size 'N'\n",
    "N = 1000\n",
    "\n",
    "# Now grow the graph\n",
    "while B.vcount() < N:\n",
    "    p = pref_attach_prob(B)\n",
    "    B.add_vertex()\n",
    "    for i in range(m):\n",
    "        r = random()\n",
    "        #print(r)\n",
    "        for j in range(B.vcount()-1):\n",
    "            if r < p[j]:\n",
    "                #print('Adding edge [%d - %d]' % (j, B.vcount()-1))\n",
    "                B.add_edge(j, B.vcount()-1)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = B.degree()\n",
    "print('The average degree is [%d].' % mean(degrees))\n",
    "print('The maximum degree is [%d].' % max(degrees))\n",
    "print('This graph has [%d] links.' % B.ecount())\n",
    "\n",
    "# It is known the degree distribution will follow a power law, so we use a log-log plot\n",
    "bins = np.logspace(np.log10(min(degrees)), np.log10(max(degrees)), num = 25)\n",
    "n, bins, hist = plt.hist(degrees, bins=bins, normed=1,log = True)\n",
    "plt.xscale('log')\n",
    "\n",
    "# Plot the function P(k) ~ k^(-3)\n",
    "y = [50*math.pow(x, -3) for x in bins]\n",
    "l = plt.plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "plt.xlabel('Degree (k)')\n",
    "plt.ylabel('Frequency P(k)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some centrality measures\n",
    "\n",
    "# There are many...\n",
    "# Degree centrality, closeness centrality, betweenness centrality, \n",
    "# eigenvector centrality, Katz centrality, PageRank centrality,\n",
    "# percolation centrality, etc. (the list goes on...)\n",
    "# Look at Wikipedia or a textbook for a quick refresher!\n",
    "\n",
    "# They each tell you something different about the structural connectivity\n",
    "# so in practice, you likely use different measures for different networks\n",
    "\n",
    "# First, look at the closeness\n",
    "# This measures the reciprocal mean shortest path from each node to each other node\n",
    "# High closeness = better graph connectivity!\n",
    "c_G = G.closeness()\n",
    "c_B = B.closeness()\n",
    "plt.hist(c_B, bins=30) # Blue = Barabasi-Albert\n",
    "plt.hist(c_G, bins=30) # Orange = Erdős-Renyi\n",
    "plt.xlabel('Closeness (c)')\n",
    "plt.ylabel('Frequency P(c)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now look at the betweenness\n",
    "# This measures the number of times a certain node is part of a \n",
    "# shortest path between two other nodes - important for routing!\n",
    "\n",
    "b_G = G.betweenness()\n",
    "b_B = B.betweenness()\n",
    "\n",
    "plt.hist(b_G, bins=30)\n",
    "plt.xlabel('Betweenness (b)')\n",
    "plt.ylabel('Frequency P(b)')\n",
    "plt.title('Erdős-Renyi')\n",
    "plt.show()\n",
    "\n",
    "bins = np.logspace(np.log10(min(b_B)), np.log10(max(b_B)), num = 25)\n",
    "plt.hist(b_B, bins=bins,log='True')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Betweenness (b)')\n",
    "plt.ylabel('Frequency P(b)')\n",
    "plt.title('Barabasi-Albert')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we will look at Google's PageRank centrality (Brin & Page, 1998)\n",
    "# This gives the stationary state of the random walk!\n",
    "\n",
    "p_G = G.pagerank()\n",
    "p_B = B.pagerank()\n",
    "\n",
    "plt.hist(p_G, bins=30)\n",
    "plt.xlabel('Pagerank (p)')\n",
    "plt.ylabel('Frequency P(p)')\n",
    "plt.title('Erdős-Renyi')\n",
    "plt.show()\n",
    "\n",
    "bins = np.logspace(np.log10(min(p_B)), np.log10(max(p_B)), num = 25)\n",
    "plt.hist(p_B, bins=bins,log='True')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Betweenness (p)')\n",
    "plt.ylabel('Frequency P(p)')\n",
    "plt.title('Barabasi-Albert')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See http://igraph.org/python/doc/igraph.Graph-class.html for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at some real data!\n",
    "# The network of airports can be found at http://openflights.org\n",
    "\n",
    "A = Graph()\n",
    "\n",
    "nodes = open('airline_nodes.dat','r')\n",
    "lines = nodes.readlines()\n",
    "for line in lines:\n",
    "    A.add_vertex(line.rstrip())\n",
    "nodes.close()\n",
    "\n",
    "edg = []\n",
    "edges = open('airline_edges.dat','r')\n",
    "lines = edges.readlines()\n",
    "for line in lines:\n",
    "    e1, e2 = line.rstrip().split(',')\n",
    "    edg += [(e1, e2)]\n",
    "A.add_edges(edg)\n",
    "    \n",
    "print('There are [%d] airports in this network.' % A.vcount())\n",
    "# print(A.vs['name']) # This is how to access vertex names\n",
    "print('There are [%d] links in this network.' % A.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place some number of people at each airport\n",
    "P = 100\n",
    "p = [P for x in range(A.vcount())] # This contains the number of people at each node (airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion Algorithm:\n",
    "# Look at 'T' timesteps:\n",
    "T = 1000\n",
    "\n",
    "# At each step, a fraction 'alpha' of people at a node move to neighbors\n",
    "alpha = 0.5\n",
    "\n",
    "# They are divided according to the degree of each neighbor\n",
    "def diffuse_probs(g, idx): #g is the graph; idx is the node we are looking at\n",
    "    n = g.neighbors(g.vs[idx])\n",
    "    d = g.degree(n)\n",
    "    return n, [x / sum(d) for x in d]\n",
    "\n",
    "# Begin the diffusion\n",
    "def diffuse(p):\n",
    "    for t in range(T):\n",
    "        i = int(random() * A.vcount()) # Pick a random node whose people will diffuse\n",
    "        #print('Looking at node [%d].' % i)\n",
    "        n, dp = diffuse_probs(A, i) # Diffusion coefficients\n",
    "        n_p = int(p[i] * alpha) # Number of people which will move\n",
    "\n",
    "        NP = [int(n_p * dp[j]) for j in range(len(n)-1)] # Number moving to each neighbor\n",
    "        NP.append(n_p - sum(NP)) # The final move (to avoid rounding errors)\n",
    "\n",
    "        p[i] -= n_p # Decrement people at this node\n",
    "\n",
    "        for j in range(len(n)):\n",
    "            p[n[j]] += NP[j] # Increment people at neighbors\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 100\n",
    "plt.hist(diffuse(P), bins=30,normed='True')\n",
    "plt.xlabel('People (p)')\n",
    "plt.ylabel('Frequency P(p)')\n",
    "plt.title('Airline Network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is an example of graph assortativity and 'rich club' behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use an SIR model of epidemics to characterize disease outbreak\n",
    "# S - susceptible to infection\n",
    "# I - infected\n",
    "# R - recovered; no longer susceptible\n",
    "\n",
    "# Optional variations:\n",
    "# D - dead!\n",
    "# IM - immune from the start\n",
    "# T - treated/immunized\n",
    "# E - exposed (infected but not infectious)\n",
    "\n",
    "# Everyone starts out susceptible\n",
    "S = [P for x in range(A.vcount())]\n",
    "# Nobody is infected, recovered, or dead yet\n",
    "I = [0 for x in range(A.vcount())]\n",
    "R = [0 for x in range(A.vcount())]\n",
    "D = [0 for x in range(A.vcount())]\n",
    "\n",
    "S_I = 0.8 # Chance susceptible individual becomes infected\n",
    "I_R = 0.7 # Chance infected individual recovers\n",
    "I_D = 0.1 # Chance infected invididual dies\n",
    "\n",
    "# Questions:\n",
    "# 1. How many days (timesteps) until nobody is infected?\n",
    "# 2. How many people die?\n",
    "\n",
    "# Outbreak!\n",
    "outbreak = int(random() * A.vcount()) # Random node where outbreak occurs\n",
    "#print('The outbreak occured at node [%d].' % outbreak)\n",
    "I0 = 0.1 # Fraction of people initially infected\n",
    "I[outbreak] = S[outbreak] * I0\n",
    "S[outbreak] -= I[outbreak]\n",
    "\n",
    "def disease_step(A, s, i, r, d, s_i, i_r, i_d):\n",
    "    for x in range(A.vcount()): # Iterate through each airport (node)\n",
    "        if s[x] > 0: # If there are susceptible individuals, they can get infected\n",
    "            rand = np.random.poisson(s_i * s[x])\n",
    "            rand = max(0, min(rand, s[x])) # Clamp the value\n",
    "            s[x] -= rand\n",
    "            i[x] += rand\n",
    "        if i[x] > 0: # If there are infected individuals, they can recover\n",
    "            rand = np.random.poisson(i_r * i[x])\n",
    "            rand = max(0, min(rand, i[x]))\n",
    "            i[x] -= rand\n",
    "            r[x] += rand\n",
    "        if i[x] > 0: # If there are infected individuals, they can also die\n",
    "            rand = np.random.poisson(i_d * i[x])\n",
    "            rand = max(0, min(rand, i[x]))\n",
    "            i[x] -= rand\n",
    "            d[x] += rand\n",
    "    return [s, i, r, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run out simulation until no more infected remain\n",
    "step = 0\n",
    "while sum(I) > 0:\n",
    "    #print(sum(S), sum(I), sum(R), sum(D))\n",
    "    S, I, R, D = disease_step(A, S, I, R, D, S_I, I_R, I_D)\n",
    "    step += 1\n",
    "#print(sum(S), sum(I), sum(R), sum(D))\n",
    "print('The epidemic lasted [%d] timesteps.' % step)\n",
    "print('[%d] people (%f%%) died.' % (sum(D), 100*float(sum(D)) / (P * A.vcount())))\n",
    "print('[%d] people never got sick.' % sum(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One can combine the diffusion with the SIRD infection process to then model a more realistic model\n",
    "# of epidemic spreading... these models can become very involved with many variables\n",
    "\n",
    "# Age distributions, geographical factors (e.g., treatment quality or climate)\n",
    "# Certain combinations of diseases can result in partial immunity, or they can kill faster together\n",
    "# Summary: using graph data structures, you are just doing a big Monte Carlo simulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
